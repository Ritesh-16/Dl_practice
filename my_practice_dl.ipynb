{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Practical 1 Write a program and preprocess the data in python.\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Your dataset\n",
    "data = {\n",
    "'Country': ['India', 'Sri Lanka', 'China', 'Sri Lanka', 'China', 'India', 'Sri Lanka', 'India', 'China', 'India', 'Sri Lanka', 'China', 'India', 'India', 'Sri Lanka'],\n",
    "'Age': [34, 22, 31, 29, 55, 24, 28, None, 51, 44, 21, 25, 33, 42, 33],\n",
    "'Salary': [92000, 25000, 74000, None, 98000, 30000, 40000, 60000, 89000, 78000, 20000, 30000, 45000, 65000, 22000],\n",
    "'Purchased': ['Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'No', 'No', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "# Separate numerical and categorical columns\n",
    "numeric_features = ['Age', 'Salary']\n",
    "categorical_features = ['Country']\n",
    "# Define preprocessing steps\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "('imputer', SimpleImputer(strategy='mean')),\n",
    "('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))])\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "transformers=[('num', numeric_transformer, numeric_features),\n",
    "('cat', categorical_transformer, categorical_features)])\n",
    "# Apply preprocessing to the data\n",
    "preprocessed_data = preprocessor.fit_transform(df)\n",
    "# Get feature names after one-hot encoding\n",
    "feature_names = numeric_features + list(preprocessor.named_transformers_['cat']\n",
    ".named_steps['onehot']\n",
    ".get_feature_names_out(categorical_features))\n",
    "# Convert preprocessed data back to a DataFrame\n",
    "preprocessed_df = pd.DataFrame(preprocessed_data, columns=feature_names)\n",
    "print(preprocessed_df)\n",
    "#Preprocessing the data\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Assuming you have your preprocessed data in preprocessed_df\n",
    "# Visualizing Age distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(preprocessed_df['Age'], bins=20, kde=True, color='skyblue')\n",
    "plt.title('Distribution of Age')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "# Visualizing Salary distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(preprocessed_df['Salary'], bins=20, kde=True, color='salmon')\n",
    "plt.title('Distribution of Salary')\n",
    "plt.xlabel('Salary')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "# Visualizing relationship between Age and Salary\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(x='Age', y='Salary', data=preprocessed_df, color='green')\n",
    "plt.title('Relationship between Age and Salary')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Salary')\n",
    "plt.show()\n",
    "# Visualizing categorical data (Country after one-hot encoding)\n",
    "country_columns = [col for col in preprocessed_df.columns if 'Country' in col]\n",
    "country_data = preprocessed_df[country_columns]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=country_data)\n",
    "plt.title('Count of Countries')\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Practical 2 Write a program to demonstrate various operations related to tensor, matrix & vector.\n",
    "\n",
    "import numpy as np\n",
    "scalar=5\n",
    "tensor_1d=np.array([1,2,3])\n",
    "tensor_2d=np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "add=np.add([1,2,3],[4,5,6])\n",
    "mul=np.multiply([1,2,3],[1,2,3])\n",
    "sub=np.subtract([2,1,2],[7,8,6])\n",
    "transpose=np.transpose([[1,2,3],[4,5,6]])\n",
    "dot=np.dot([1,2,3],[4,5,6])\n",
    "zeros=np.zeros(4)\n",
    "unit=np.eye(4)\n",
    "unit\n",
    "\n",
    "import tensorflow as tf\n",
    "m=tf.constant([[1,2,3],[4,5,6],[7,8,9]], shape=(3,3))\n",
    "tf.Tensor([[1 ,2 ,3],[4, 5 ,6],[7, 8 ,9]], shape=(3, 3), dtype=int32)\n",
    "n=tf.constant([[10,20,30],[40,50,60],[70,80,90]],shape=(3,3))\n",
    "\n",
    "tf.Tensor(\n",
    "[[10, 20, 30]\n",
    "[40, 50 ,60]\n",
    "[70 ,80 ,90]], shape=(3, 3), dtype=int32)\n",
    "add1=tf.add(m,n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical 3 Perfrom the following matrix operation in python:\n",
    "# 1. Finding eigen values and eigen vectors\n",
    "# 2. Check for linear denpendent and linear independent\n",
    "# 3. Perform matrix transpose\n",
    "# 4. Perform diagnonal martix\n",
    "# 5. Perform triangular marix\n",
    "# 6. Perform orthogonal matrix using numpy modules\n",
    "\n",
    "#1. Finding eigen values and eigen vectors\n",
    "import numpy as np\n",
    "matrix_a=np.array([[4,6,6],[1,3,2],[-1,-4,-3]])\n",
    "#finding eigen values and eigenvectors\n",
    "eigen_values,eigen_vectors=np.linalg.eig(matrix_a)\n",
    "eigen_values,eigen_vectors\n",
    "#checking for Linear Independence\n",
    "rank = np.linalg.matrix_rank(matrix_a)\n",
    "rows,columns=matrix_a.shape\n",
    "if rank==min(rows,columns):\n",
    "  print(\"Independent\")\n",
    "else:\n",
    "  print(\"Dependent\")\n",
    "\n",
    "#transpose\n",
    "np.transpose(matrix_a)\n",
    "\n",
    "#diagonal matrix check\n",
    "if sum(np.diag(matrix_a))==np.sum(matrix_a):\n",
    "  print(\"Diagonal Matrix\")\n",
    "else:\n",
    "  print(\"Not Diagonal\")\n",
    "\n",
    "#triangular matrix\n",
    "if np.allclose(np.triu(matrix_a), matrix_a):\n",
    "    print(\"The matrix is upper triangular.\")\n",
    "elif np.allclose(np.tril(matrix_a), matrix_a):\n",
    "    print(\"The matrix is lower triangular.\")\n",
    "else:\n",
    "    print(\"The matrix is not triangular.\")\n",
    "\n",
    "#orthogonal matrix\n",
    "matrix_a=[[0,1],[-1,0]]\n",
    "product=np.dot(matrix_a,np.transpose(matrix_a))\n",
    "unit_matrix=np.eye(len(matrix_a))\n",
    "len(matrix_a)\n",
    "if np.allclose(product,unit_matrix):\n",
    "  print(\"Orthogonal\")\n",
    "else:\n",
    "  print(\"Not Orthogonal\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Practical 4 : Solve the EXOR problem using Deep learning\n",
    "\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "x=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y=np.array([[0],[1],[1],[0]])\n",
    "\n",
    "model=Sequential([\n",
    "    Dense(8,input_dim=2,activation=\"relu\"),\n",
    "    Dense(4,activation=\"relu\"),\n",
    "    Dropout(0.1),\n",
    "    Dense(1,activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "output=model.fit(x,y,epochs=20,steps_per_epoch=3)\n",
    "\n",
    "predict=model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Practical 5:Performing Convolutional operation using CNN\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Activation\n",
    "\n",
    "img=cv2.imread(\"//content//dog.jpg\",cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "img_4d=img.reshape(1,img.shape[0],img.shape[1],1)\n",
    "img_4d.shape\n",
    "\n",
    "model1=Sequential([\n",
    "    Conv2D(1,(1,1),padding=\"valid\",input_shape=img_4d.shape[1:])\n",
    "])\n",
    "\n",
    "conv_img1=model1.predict(img_4d)\n",
    "conv_img1_show=conv_img1.reshape(conv_img1.shape[1],conv_img1.shape[2])\n",
    "plt.imshow(conv_img1_show,cmap='gray')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Practical 6 : Write a program to implement convolution with various layers\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.layers import Dense,Conv2D,Dropout,MaxPooling2D,Flatten\n",
    "from keras.models import Sequential\n",
    "\n",
    "fashion_data=fashion_mnist.load_data()\n",
    "(x_train,y_train),(x_test,y_test)=fashion_data\n",
    "x_train,x_validate=x_train[:50000],x_train[50000:]\n",
    "y_train,y_validate=y_train[:50000],y_train[50000:]\n",
    "\n",
    "class_name=[\"T-Shirt/Top\",\"Trousers\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag\",\"Ankleboot\"]\n",
    "model=Sequential([\n",
    "    Conv2D(64,3,activation=\"relu\",padding=\"same\",input_shape=[28,28,1]),\n",
    "    MaxPooling2D(2),\n",
    "    Dense(22,activation=\"relu\"),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(10,activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",metrics=['accuracy'],optimizer=\"adam\")\n",
    "output=model.fit(x_train,y_train,epochs=2,steps_per_epoch=5,validation_data=(x_validate,y_validate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Practical 7 : CNN training parameters\n",
    "\n",
    "plt.plot(output.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(output.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy Over Epochs')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(output.history['loss'], label='Training Loss')\n",
    "plt.plot(output.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Practical 8 Implementation of CNN to Predict numbers from Number Images.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train,y_train),(x_test,y_test)=mnist.load_data()\n",
    "\n",
    "model=Sequential([\n",
    "    Conv2D(1,(3,3),padding=\"valid\",input_shape=(28,28,1)),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(10,activation=\"softmax\")\n",
    "])\n",
    "x_train.shape\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "model.fit(x_train,y_train,epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Practical 9 Write a Program to implement RNN\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the data for Simple RNN (each sample is treated as a sequence of features)\n",
    "X_train_reshaped = X_train_scaled[:, :, None]  # Add a third dimension\n",
    "X_test_reshaped = X_test_scaled[:, :, None]\n",
    "\n",
    "# Define the Simple RNN model\n",
    "model = Sequential([\n",
    "    SimpleRNN(32, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_reshaped, y_train, epochs=20, batch_size=32, validation_data=(X_test_reshaped, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
